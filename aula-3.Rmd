---
title: "Técnicas Computacionais em Estatística"
output: html_notebook
date: "21 de Março de 2018"
---

# Aula 3

**Exemplo:**
Usar o método da transformada inversa para gerar uma amostra de $n$ valores pseudo-aleatórios do modelo logarítmico ($\theta = 0.5$)
$$
	f_X(x) = P(X = x) = \frac{a \theta^x}{x},
$$
$x = 1, 2, 3, \dotsc$, $0 < \theta < 1$, $a = -\frac{1}{\log(1 - \theta)}$.
$$
	f_X(x) = \exp(\log(a) + x \log(\theta) - \log(x))
$$
guardar num vetor os
$$
	F_X(x),\ x = 1, 2, \dotsc, N\ \text{(elevado)}.
$$
Resolver $F_X(x - 1) < u \leq F_X(x)$.

Para resolver $F_X(x - 1) < u \le F_X(x)$ para um dado $u$ é necessário contar o número de valores $x$ dos quais
$$
	F_X(x - 1) < u.
$$

A soma dos valores 1 que indicam que no vetor a relação de `TRUE` é exatamente o número de $x - 1$

```{r}
theta <- 0.5
u <- runif(1)
k <- 1:50
a <- -1 / log(1 - theta)
fk <- exp(log(a) + k * log(theta) - log(k))
Fk <- cumsum(fk)

Fk
plot(Fk, ylim = 0:1)
```

```{r}
# Função para gerar NPAs logarítmica(theta)

rlogc <- function(n, theta) {
	u <- runif(n)
	N <- ceiling(-16 / log10(theta))
	k <- 1:N
	a <- -1 / log(1 - theta)
	fk <- exp(log(a) + k * log(theta) - log(k))
	Fk <- cumsum(fk)
	x <- integer(n)
	for (i in 1:n) {
		x[i] <- as.integer(sum(u[i] > Fk))
		while (x[i] == N) {
			logf <- log(a) + (N + 1) * log(theta) - log(N + 1)
			fk <- c(fk, exp(logf))
			Fk <- c(Fk, Fk[N] + fk[N + 1])
			N <- N + 1
			x[i] <- as.integer(sum(u[i] > Fk))
		}
	}
	return(x + 1)
}

n <- 1000
theta <- 0.5
x <- rlogc(n, theta)
k <- sort(unique(x))

fmp <- -1 / log(1 - theta) * theta ^ k / k
freq <- table(x) / n
round(rbind(freq, fmp), 3)

plot(freq)
```


### Método da Transformação - Distribuição Poisson($\lambda$)

**Proposição:**
Dados $\{X_i\}, i \geq 1$, NPA's exponenciais unitários independentes.

$N :=$ "maior inteiro tal que $\sum_{i = 1}^N X_i \leq \lambda$".

**Algoritmo:**

	Entrada:
		lambda
	
	1: Faça-se S1 = 0
	2: Para i = 1, 2, ..., enquanto Si <= 1
		(a) gerar Ti ~ Exponencial
		(b) Seja Si = T1 + ... + Ti
		
	Saída:
		N := min(Si > 1) - 1
		
Propriedade da distribuição Exponencial:
$$
	X \sim Exp(\lambda) \\
	kX \sim Exp \left( \frac{\lambda}{k} \right)
$$

```{r}
rpoisson <- function(n, lambda) {
	N <- integer(n)
	for (i in 1:n) {
		Tn <- -log(runif(100)) / lambda
		Sn <- cumsum(Tn)
		N[i] <- min(which(Sn > 1)) - 1
	}
	return(N)
}

n <- 1000
lambda <- 5
x <- rpoisson(n, lambda)
plot(table(x) / n)
```


### Modelo de Rejeição - Modelo Truncado

**Definição (Variável Aleatória Truncada):**
Seja $X$ uma v.a. com f.d. $F(\cdot)$ e f.d.p. $f(\cdot)$.
A v.a., definida por
$$
	X_{[a, b]} := X \mid a \leq X \leq b
$$
designa-se v.a. truncada à esquerda de $a$ e à direita de $b$, com f.d.p.
$$
	f_{[a, b]}(x) = \frac{f(x)}{F(b) - F(a)},
$$
para $x \in [a, b]$.


#### Geração de um modelo truncado
**Algoritmo:**

	Repita
		gerar x NPA com f.d. F()
		até que x in [a, b]
		
	Saída
		xab := x

**Exemplo:**
NPA's N(0, 1) truncadas em [a, b]
```{r}
a <- -3
b <- 3
n <- 1000
k <- 0  # contador par aas observações aceitas
j <- 0  # iterações
y <- numeric(n)

while (k < n) {
	x <- rnorm(1)  # valores da N(0, 1)
	j <- j + 1
	if (x >= a & x <= b) {
		# aceita x
		k <- k + 1
		y[k] <- x
	}
}

hist(y, prob = TRUE)
```


### Método de Rejeição-Aceitação

O Método de Rejeição-Aceitação é um método que requer apenas que saibermos gerar de uma densidade instrumental $g$ (ou candidata) para gerar de $f$ (densidade alvo).

As únicas restrições que pensamos impor são

1. $f$ e $g$ tenham suporte compatíveis.

2. Há uma constante $M$ com $\frac{f(x)}{g(x)} \leq M,\ \forall x$.

Nesse caso, $X$ pode ser simulado da seguinte forma:
geramos $V \sim g$ e, independentemente, geramos $U \sim U[0, 1]$. Se
$$
	U \leq \frac{1}{M} \frac{f(Y)}{g(Y)},
$$
então $X = Y$.
Se a desigualdade nãp for satisfeita, descartamos $Y$.

**Algortimo (Aceitação-Rejeição)**

	1. Gera Y ~ g, U ~ U[0, 1]
	2. Aceita X = Y se U <= 1/M * f(Y)/g(Y)
	3. Retoma para 1 caso contrário

$$
	P\left(Y \leq x \mid U \leq \frac{1}{M} \frac{f(Y)}{g(Y)}\right) = P(X \leq x) \\
$$ $$
	= \frac{P(Y \leq x, U \leq \frac{1}{M} \frac{f(Y)}{g(Y)})}{P(U \leq \frac{1}{M} \frac{f(Y)}{g(Y)})}
$$ $$
	= \frac{\int_{-\infty}^x \int_0^{\frac{1}{M} \frac{f(Y)}{g(Y)}} du\ g(y)\ dy}{\int_{-\infty}^\infty \int_0^{\frac{1}{M} \frac{f(Y)}{g(Y)}} du\ g(y)\ dy}
$$
$$
	= \frac{\int_{-\infty}^x \frac{1}{M} \frac{f(Y)}{g(Y)} g(y)\ dy}{\int_{-\infty}^{\infty} \frac{1}{M} \frac{f(Y)}{g(Y)} g(y)\ dy}
$$
$$
	= \frac{\int_{-\infty}^x f(y)\ dy}{\int_{-\infty}^\infty f(y)\ dy} = P(X \leq x)
$$

**Exemplo:**
Gerar de uma Beta($\alpha$, $\beta$)
$\alpha = 2.7$, $\beta = 6.3$

Distribuição candidata: U[0, 1]

$M?$ no R, pois achamos $M$ podemos fazer o seguinte
```{r}
optimize(f = function(x){dbeta(x, 2.7, 6.3)}, interval = c(0, 1), maximum = TRUE)$objective

Nsim <- 2500
a <- 2.7
b <- 6.3
M <- 2.7

u <- runif(Nsim, max = M)
y <- runif(Nsim)

x <- y[u < dbeta(y, a, b)]

hist(x, probability = TRUE)
```


### Método da Transformação

Muitos tipos de transformação além da transformação inversa podem ser aplicadas para a simulação de v.a.'s.

Resultados mais comuns:

- Se $Z \sim N(0, 1)$, então $U = Z^2 \sim \chi_{(1)}^2$

- Se $V \sim \chi_{(m)}^2$, e $W \sim \chi_{(n)}^2$, e independentes, então $F = \frac{V / m}{W / n} \sim F_{m, n}$

- Se $Z \sim N(0, 1)$ e $V \sim \chi_{(n)}^2$ independentesm então $T = \frac{Z}{\sqrt{V / n}} \sim t(n)$

- Se $U, V \sim U[0, 1]$ independentes, então $Z_1 = \sqrt{-2 \log U} \cdot \cos(2 \pi V)$, $Z_2 = \sqrt{-2 \log U} \cdot \sin(2 \pi V)$ são independentes $N(0, 1)$ (Algoritmo Box-Muller)

- Se $U, V \sim U[0, 1]$ independentes, então $\lfloor 1 + \frac{\log(V)}{\log (1 + (1 - \theta)^U)} \rfloor$ tem distribuição logarítmica(t), em que $\lfloor x \rfloor$ denota a parte inteira de $x$

- Se $U \sim Gamma(a, \beta)$, e $V \sim Gamma(b, \beta)$, independentes, então $\frac{U}{U + V} \sim Beta(a, b)$