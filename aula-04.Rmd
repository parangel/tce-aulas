---
title: "Técnicas Computacionais em Estatística"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "28 de Março de 2018"
---

# Aula 4

## Somas e Misturas

**Exemplo:**

Geram $n = 1000$ NPA's $\chi^2(\nu)$ através da convolução de $\nu$ quadrados da normal padrão
$$
	V = \sum_{i = 1}^{\nu}Z_i^2 \sim \chi^2(\nu)
$$

No R:

```{r}
n  <- 1000
nu <- 2
X <- matrix(rnorm(n * nu), n, nu) ^ 2
y <- rowSums(X)
# y <- apply(X, MARGIN = 1, sum)

mean(y)
var(y)

```

### Somas e Misturas Discreto

Uma v.a. X diz-se que é uma mistura discreta de modelos se
$$
	F_X(x) = \sum_{i = 1}^{k} \theta_i F_i(x)
$$
com $\theta_i > 0$ e
$$
	\sum_{i = 1}^k \theta_i = 1.
$$


**Exemplo:**

Gerar $n = 1000$ NPA's para o modelo $X := \frac{1}{3}X_1 + \frac{2}{3}X_2$, $X_1 \sim N(0, 1)$, $X_2 \sim N(3, 1)$.

Algoritmo para geração:

	1. Gerar um inteiro k in {1, 2} em que P(1) = 1/3 e P(2) = 2/3
	2. Se k = 1 saída X NPA do modelo N(0, 1)
	   Se k = 1 saída X NPA do modelo N(3, 1)

No R:

```{r}
n <- 1000
mu <- c(0, 3)
k <- sample(1:2, size = n, replace = TRUE, prob = c(1 / 3, 2 / 3))
m <- mu[k]

x <- rnorm(n, m, 1)

plot(density(x), xlim = c(-6, 6), ylim = c(0, .5), lwd = 3,
	 xlab = "x", main = "", col = "grey20")
text(1.9, 0.05, "1000 NPA's mistura")
for (i in 1:2) {
	lines(density(rnorm(n, mu[i], 1)), lty = 2)
}
text(-2.5, 0.3, "fdp N(0, 1)")
text(4.5, 0.3, "fdp N(3, 1)")

hist(x, prob = TRUE, main = NULL, ylim = c(0, 0.4), col = "grey20")
text(0, 0.38, "Histograma da Mistura")

```

##  Métodos de Monte Carlo

Genericamente, são designados por métodos de Monte Carlo todos os métodos computacionais onde se examinam as propriedades de uma distribuição de probabilidade, gerando várias amostras e estudando as respectivas propriedades estatísticas.

Num primeiro momento, estamos interessados em
$$
	E[g(X)]
$$
para uma função real $g$.

Seja $X$ uma v.a. e $g$ uma função real.

- Podemos encontrar analiticamente
$$
	E[g(X)] = \int g(x) f(x)\ dx
$$
- Podemos encontrar usando métodos numéricos (trapézios)

- Podemos usar "Integração de Monte Carlo".
Essa técnica é baseada na Lei Forte dos Grandes Números.
Se $\{X_i\}_{i \in \mathbb{N}}$ é uma sucessão de v.a.'s iid, então
$$
	\lim_{N \to \infty} \sum_{i = 1}^{N} = E[g(X)]
$$
quase certamente.

Na prática, podemos usar um $N$ relativamente grande, ou seja,
$$
	E[g(X)] \approx \frac{1}{N} \sum_{i = 1}^{N} g(X_i).
$$


**Exemplo:**

Calcular a estimativa de Monte Carlo para
$$
	\theta = \int_0^1 e^{-x}\ dx.
$$
Como faríamos no R:
```{r}
n <- 10000
theta = mean(exp(-runif(n)))
theta
```

Podemos também calcular probabilidades usando a integração de Monte Carlo, reescrevendo como valores médios:
se $X$ é uma v.a. e $X_i$ iid a $X$, então para $N$ elevado
$$
	P[X \in A] = E[I_A(X)] \approx \frac{1}{N} \sum_{i = 1}^{N}  I_A(X_i).
$$

Podemos também usar integrais definidas de tipo
$$
	\int_{a}^{b} g(x)\ dx
$$
Sejam $U_i$ iid a $U[a, b]$,
$$
	f_U(x) = \frac{1}{b - a} I_{[a, b]}(x).
$$
$$
	\int_{a}^{b} g(x)\ dx = (b - a) \int g(x) f_U(x)\ dx = (b - a) E[g(U)]
$$


**Exemplo:**

$$
	\theta = \int_2^4 e^{-x}\ dx
$$
```{r}
m <- 10000
x <- runif(m, 2, 4)
theta <- mean(exp(-x)) * (4 - 2)
theta
```


**Exemplo:**

$X \sim N(0, 1)$, $P(X > 4.5)$

```{r}
a <- 4.5
for (i in 5:7) {
	N <- 10 ^ i
	z <- rnorm(N)
	p.hat <- sum(z > a) / N
	p <- pnorm(-a)
	dif <- abs(sum(z > a) / N - p)
	# mylist <- list(N = N, p.hat = p.hat, p = p, dif = dif)
	# print(mylist)
	print(c(N = N, p.hat = p.hat, p = p, dif = dif))
}
```


### Precisão da Estimativa MC

O estimador de MC é centrado
$$
	E\left[ \frac{1}{N} \sum_{i = 1}^{N}g(X_i) \right] = \frac{1}{N} \sum_{i = 1}^N E[g(X_i)] = E[g(X)]
 $$

A variância para o estimador de MC é dado por
$$
	Var \left[ \frac{1}{N} \sum_{i = 1}^N g(X_i) \right] = \frac{1}{N^2} \sum^N Var[g(X_i)] = \frac{1}{N^2} N Var[g(X_i)] = \frac{1}{N} Var[g(X_i)]
$$

- Variância vai para $0$ quando $N \to \infty$

- Consistência decaindo para zero a uma velocidade de $1/\sqrt{N}$


Para determinar o valor N necessário para atingir uma determinada precisão, pode-se proceder da seguinte forma:
pela desigualdade de Chebyshev,
$$
	P\left[ \left| \frac{1}{N} \sum_{i = 1}^{N}g(X_i) - E[g(X)] \right| \leq \epsilon \right] \geq 1 - \frac{Var[g(X)]}{N \epsilon^2} \Rightarrow N \geq \frac{Var[g(X)]}{\alpha \epsilon^2}
$$


**Exemplo:**

Vamos assumir que  existe um majorante para a variância $Var[g(X)] \leq 1$. $\epsilon = 0.01$, $\alpha = 0.05$
$$N \geq \frac{1}{\alpha \epsilon^2} = \frac{1}{0.05 (0.01)^2} = 200000$$


### Aplicações em Inferência

Em inferência, normalmente temos uma amostra $X_1, \dotsc, X_n$ que se presume ser gerada de acordo com algum modelo estatístico.
O nosso objetivo é obter informações sobre os parâmetros do modelo subjacente aos dados.

Seja $T = T(X_1, \dotsc, X_n)$ uma estatística.
Um exemplo importante de uma estatística é o caso onde $T$ é usado para estimar um parâmetro, nesse caso $T$ é um estimador.

Frequentemente, em especial para $n$ pequeno, a distribuição de $T$ não é conhecida explicitamente, mas podemos estudá-lo usando método de Monte Carlo.